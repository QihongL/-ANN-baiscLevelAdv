ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(fun.y=mean, geom="line")
#make orthogonal polynomial
t <- poly(1:10, 2)
t
#it can be a good idea to pull the range directly from your data set
t <- poly(1:max(WordLearnEx$Block), 2)
t
#add it into the data frame
WordLearnEx[, paste("ot", 1:2, sep="")] <- t[WordLearnEx$Block, 1:2]
#re-check data
summary(WordLearnEx)
#orthogonal polynomial time
ggplot(WordLearnEx, aes(Block, ot1)) + stat_summary(fun.y=mean, geom="line")
last_plot() + stat_summary(aes(y=ot2), fun.y=mean, geom="line", color="red")
#fit base model
m.base <- lmer(Accuracy ~ (ot1+ot2) + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
#add effect of TP on intercept
m.0 <- lmer(Accuracy ~ (ot1+ot2) + TP + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
#add effect on slope
m.1 <- lmer(Accuracy ~ (ot1+ot2) + TP + TP:ot1 + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
#add effect on quadratic
m.2 <- lmer(Accuracy ~ (ot1+ot2)*TP + (ot1+ot2 | Subject), data=WordLearnEx, REML=F)
#model comparisons
anova(m.base, m.0, m.1, m.2)
#plot model fit
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
#parameter estimates
summary(m.2)
coefs <- data.frame(coef(summary(m.2)))
#parameter-specific p-values: use normal approximation
coefs$p <- 2*(1-pnorm(abs(coefs$t.value)))
coefs
#Alternative: use lmerTest to get Satterthwaite approximation
library(lmerTest)
# lmerTest will take over the lmer function
# Pro: you can use the same model-fitting code
m.2t <- lmer(Accuracy ~ (ot1+ot2)*TP + (ot1+ot2 | Subject), data=WordLearnEx, REML=F)
summary(m.2t)
# Con: your model is now a lmerTest object rather than a lmerMod object and has somewhat different behavior
detach("package:lmerTest", unload=T)
# 9. MORE ABOUT RANDOM EFFECTS
#    (a) WISQARS data: different random effects structures example
#    (b) Keep it maximal
#    (c) convergence problems can sometimes be addressed by simplifying the random effects structure
ggplot(TargetFix, aes(Time, meanFix, color=Condition)) +
stat_summary(fun.y=mean, geom="line") +
stat_summary(aes(fill=Condition), fun.data=mean_se, geom="ribbon", color=NA, alpha=0.3) +
theme_bw(base_size=10) + expand_limits(y=c(0,1)) +
labs(y="Fixation Proportion", x="Time since word onset (ms)")
t <- poly(1:max(TargetFix$timeBin), 3)
TargetFix[,paste("ot", 1:3, sep="")] <- t[TargetFix$timeBin, 1:3]
m.full <- lmer(meanFix ~ (ot1+ot2+ot3)*Condition + #fixed effects
(ot1+ot2+ot3 | Subject) +
(ot1+ot2+ot3 | Subject:Condition), #random effects
data=TargetFix, REML=F)
summary(m.full)
# look at random effects
str(ranef(m.full))
head(ranef(m.full)$"Subject")
head(ranef(m.full)$"Subject:Condition")
VarCorr(m.full)
# what is being estimated?
# (a) random variance and covariance
# (b) unit-level random effects
# this is why df for parameter estimates are poorly defined in MLR
#alternative random effect structure
m.alt <- lmer(meanFix ~ (ot1+ot2+ot3)*Condition + #fixed effects
((ot1+ot2+ot3)*Condition | Subject), #random effects
data=TargetFix, REML=F)
str(ranef(m.alt))
head(ranef(m.alt)$"Subject")
VarCorr(m.alt)
#this alternative version makes fewer assumptions:
# unequal variances across conditions
# more flexible covariance structure between random effect terms
#but requires more parameters...
###############
## PARTICIPANTS AS FIXED VS. RANDOM EFFECTS
# Treating participants as fixed effects produces more flexible model, perhaps too flexible
# - Shrinkage
# - Generalization
# Example:
m.pfix <- lmer(meanFix ~ (ot1+ot2+ot3)*Condition + (ot1+ot2+ot3)*Subject + #fixed effects
(ot1+ot2+ot3 | Subject:Condition), #random effects
data=TargetFix, REML=F)
#fixed effects
coef(summary(m.pfix))
#compare with participants as random effects
coef(summary(m.full))
#compare model fits
anova(m.pfix, m.full)
# Bottom line: Treating participants as random effects captures the typical assumption of random sampling from some population to which we wish to generalize. Treating participants as fixed effects can be appropriate when this is not the case (e.g., neurological case studies).
##########################
# ANALYSIS TIME
# try GCA on your own data, ask for help if you need it
# if you don't have any appropriate data on hand, try one (or more) of these exercises:
#
# Exercise 2: Categorical perception (CP: d' peak at category boundary)
#  compare categorical perception along spectral vs. temporal dimensions using second-order orthogonal polynomial
#  which terms show significant effects of dimension type? (model comparisons)
#  estimate parameter-specific p-values using normal approximation and Satterthwaite approximation (lmerTest): to what extent do model comparisons and the two parameter-specific approaches yield the same results?
#  plot observed and model fit data
#
# Exercise 3: analyze the combined effects of task difficulty and impairment (alcohol) on motor learning (MotorLearning)
#  plot the observed data
#  run a basic GCA with third-order orthogonal polynomials
#  re-code variables to get main effects instead of simple effects (i.e., set factor contrasts to "sum")
#  re-run GCA and compare results
##########################
summary(WordLearnEx)
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(fun.y=mean, geom="line")
library("minqa", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
t <- poly(1:10, 2)
t
plot(t)
t(1)
t[1]
t[1:end, 1]
t[1:, 1]
t[1:10, 1]
t <- poly(1:max(WordLearnEx$Block), 2)
t
#add it into the data frame
t
WordLearnEx[, paste("ot", 1:2, sep="")] <- t[WordLearnEx$Block, 1:2]
#re-check data
summary(WordLearnEx)
#orthogonal polynomial time
ggplot(WordLearnEx, aes(Block, ot1)) + stat_summary(fun.y=mean, geom="line")
last_plot() + stat_summary(aes(y=ot2), fun.y=mean, geom="line", color="red")
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
#parameter estimates
summary(m.2)
coefs <- data.frame(coef(summary(m.2)))
#parameter-specific p-values: use normal approximation
coefs$p <- 2*(1-pnorm(abs(coefs$t.value)))
coefs
#Alternative: use lmerTest to get Satterthwaite approximation
nal polynomial time
ggplot(WordLearnEx, aes(Block, ot1)) + stat_summary(fun.y=mean, geom="line")
last_plot() + stat_summary(aes(y=ot2), fun.y=mean, geom="line", color="red")
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(fun.y=mean, geom="line")
summary(WordLearnEx)
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(fun.y=mean, geom="line")
m.base <- lmer(Accuracy ~ (ot1+ot2) + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
m.0 <- lmer(Accuracy ~ (ot1+ot2) + TP + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
#add effect on slope
m.1 <- lmer(Accuracy ~ (ot1+ot2) + TP + TP:ot1 + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
#add effect on quadratic
m.2 <- lmer(Accuracy ~ (ot1+ot2)*TP + (ot1+ot2 | Subject), data=WordLearnEx, REML=F)
#model comparisons
anova(m.base, m.0, m.1, m.2)
#plot model fit
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
library(lmerTest)
install.packages('lmerTest')
library(lmerTest)
m.2t <- lmer(Accuracy ~ (ot1+ot2)*TP + (ot1+ot2 | Subject), data=WordLearnEx, REML=F)
summary(m.2t)
m.2t <- lmer(Accuracy ~ (ot1+ot2)*TP + (ot1+ot2 | Subject), data=WordLearnEx, REML=F)
summary(m.2t)
# Con: your model is now a lmerTest object rather than a lmerMod object and has somewhat different behavior
detach("package:lmerTest", unload=T)
source('~/Code/R/testing.R')
head(ChickWeight)
ggplot(TargetFix, aes(Time, meanFix, color=Condition)) +
stat_summary(fun.y=mean, geom="line") +
stat_summary(aes(fill=Condition), fun.data=mean_se, geom="ribbon", color=NA, alpha=0.3) +
theme_bw(base_size=10) + expand_limits(y=c(0,1)) +
labs(y="Fixation Proportion", x="Time since word onset (ms)")
ggplot(TargetFix, aes(Time, meanFix, color=Condition)) +
stat_summary(fun.y=mean, geom="line") +
stat_summary(aes(fill=Condition), fun.data=mean_se, geom="ribbon", color=NA, alpha=0.3) +
theme_bw(base_size=10) + expand_limits(y=c(0,1)) +
labs(y="Fixation Proportion", x="Time since word onset (ms)")
t <- poly(1:max(TargetFix$timeBin), 3)
#add it into data frame
TargetFix[,paste("ot", 1:3, sep="")] <- t[TargetFix$timeBin, 1:3]
#fit full model
m.full <- lmer(meanFix ~ (ot1+ot2+ot3)*Condition + #fixed effects
(ot1+ot2+ot3 | Subject) +
(ot1+ot2+ot3 | Subject:Condition), #random effects
data=TargetFix, REML=F)
summary(m.full)
install.packages('glmnet')
ver
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
}
celsius = function(212)
function(212)
celsius = function(fahrenheit) {
celsius = (fahrenheir - 32) * 5 / 9
}
stopifnot(isTRUE(all.equal(celsius(32), 0)))
celsius = function(fahrenheit) {
celsius = (fahrenheit - 32) * 5 / 9
}
stopifnot(isTRUE(all.equal(celsius(32), 0)))
stopifnot(isTRUE(all.equal(celsius(212), 100)))
celsius(32)
a = celsius(32)
a
a = celsius(212)
a
x = 1:10
x
x = (1:10, sep = 0.1)
x = seq(1,10,0.1)
x
y = 1 / (1 + exp(-x))
plot(x,y)
x = seq(-10, 10, 0.1)
plot(x,y)
x = seq(-10, 10, 0.1)
y = 1 / (1 + exp(-x))
plot(x,y)
plot(x,y,'l')
plot(x,y)
x = rnorm(100)
y = rnorm(100) + x
plot(x,y)
model = lm(y~x)
plot(model)
model
lm
a = 10
a.model = 10
a
a.mode
a.model
a
source('stats327_hw1.R')
675 * 5
675 + 1400 + 720
675 * 4
12 / 1.6
3600 + 40 * 60 + 35
10 / 0.71
2700 - 1885
x = rnorm(100)
y = rnorm(100) + x
plot(x,y)
lm(x,y)
lm(x~y)
m = lm(x~y)
abline(m)
m = lm(y ~ x)
abline(m)
plot(x,y)
abline(m, 'red')
abline(m, 'r')
x = seq(10,1)
x
x = seq(1,10,1)
x
x = seq(1,10,0.1)
x
x = seq(1,10,0.01)
y = 1 / (1 + e^(-x))
y = 1 / (1 + exp(1)^(-x))
exp(1)
plot(x,y)
x = seq(-10,10,0.01)
y = 1 / (1 + exp(1)^(-x))
plot(x,y)
? plt
? plot
plot(x,y,'l')
plot(x,y,'l')
1
1 + 1/3
1 - 1/3
1 - 1/3 + 1/5
1 - 1/3 - 1/5
(1 - 1/3 - 1/5)
4 * (1 - 1/3 - 1/5)
(1 - 1/3 - 1/5 + 1/7)
4 * (1 - 1/3 - 1/5 + 1/7)
4 * (1 - 1/3 - 1/5 + 1/7 - 1/9)
4 * (1 - 1/3 - 1/5 + 1/7 - 1/9 + 1/11)
1/ 7
f = rnorm(100)
f[3]
rep(2.3)
rep(2.3,3)
2.3 -> f[3]
f
seq(1,15,1)
5: 15
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = F)
sample(letters, 5 , replace = F)
sample(letters, 5 , replace = F)
sample(letters, 5 , replace = F)
sample(1:16, 5 , replace = T)
sample(1:16, 5 , replace = T)
c(5, letters)
c
c
c(5, letters)
c(5, letters)
c(5, letters)
c(5, letters)
c(5, letters)
sample(LETTERS, 5, replace = F)
sample(LETTERS, 5, replace = F)
sample(LETTERS, 5, replace = F)
a <- sample(letters[1:4], 100, replace=T)
a
a <- sample(letters[1:4], 100, replace=F)
a <- sample(letters[1:4], 100, replace=F)
a <- sample(letters[1:4], 100, replace=T)
table(a)
pie(table(a))
pie(a)
plot(a)
plot(table(a))
a =  14549535
a
a / 19
a / 18
a / 20
b = a
b = 692835
a
b
a / b
a
installed.packages
installed.packages()
0.0625 * 2
a = 0.0625 * 2
a * 3
1/50
1/50 * 0.06
1/50 * 0.06 * 100
install.packages(c("caTools", "class", "cluster", "codetools", "formatR", "Formula", "gplots", "Hmisc", "KernSmooth", "labeling", "lme4", "lmerTest", "markdown", "MASS", "mgcv", "mime", "multcomp", "mvtnorm", "pbkrtest", "Rcpp", "RcppEigen", "sandwich"))
#'
#' Can we assure ourselves that all the same principles apply?
#+ echo=FALSE
library(lme4)
library(dplyr)
library(AICcmodavg)
update
update()''
update()
VERSION
version
version
version
#+ echo=FALSE
library(lme4)
library(dplyr)
library(AICcmodavg)
library(ggplot2)
inta
install.packages('Matrix')
install.packages("Matrix")
#+ echo=FALSE
library(lme4)
library(dplyr)
library(AICcmodavg)
library(ggplot2)
installed.packages
installed.packages()
#+ echo=FALSE
library(lme4)
library(lme4)
library(dplyr)
library(dplyr)
library(AICcmodavg)
library(ggplot2)
#' ## Same data
df <- sleepstudy
df$Subject <- paste0("S", df$Subject) ## don't accidentally treat this as a num
raw_data <- ggplot(df, aes(x = Days, y = Reaction)) +
facet_wrap("Subject", ncol = 3) +
geom_point()
raw_data
#' ## By subject plots
#' or: "Look what ggplot can do!"
gg_magic <- raw_data +
geom_smooth(method = "lm", formula = y ~ x)
gg_magic
#' Look at the effect for each subject
40
60 - 7
53 / 3
53 / 3 * 100
53 / 3 * 5
14 * 0.71
ls
ls()
rm()
rm(ls())
? rm()
rm (list = ls())
ls
? sep()
x = 1 : 10
x
x = 1:0.1:10
x
x = 1:10, sep = 0.1
x = (1:10, sep = 0.1)
x = 1:10:0.1
x
? plot
require(stats)
plot(cars)
lines(lowess(cars))
? seq
x = seq(1,10)
x
x = seq(1,10,1)
x
x = seq(1,10,0.1)
x
x = seq(1,10,0.01)
y = 1 / (1 + (exp(-x)))
plot (x,y)
x = seq(-10,10,0.01)
y = 1 / (1 + (exp(-x)))
plot (x,y)
plot.net()
plot.new()
? plot
plot (x,y, "p")
plot.new()
plot (x,y, "l")
x = 0.45
y = 1 / (1 + (exp(-x)))
y
y + 0.08
x = y + 0.08
y = 1 / (1 + (exp(-x)))
y
x = 0.28
y = 1 / (1 + (exp(-x)))
y
x = 0.28
y = 1 / (1 + (exp(-x)))
y
bias = y
bias
x = -0.28
y = 1 / (1 + (exp(-x)))
h1 = y
x = -0.4
y = 1 / (1 + (exp(-x)))
h2 = y
h1
h1 + h2 + bias
x = (h1 + h2 + bias)
y = 1 / (1 + (exp(-x)))
y
sqrt(25)
sqrt(155)
sqrt(225)
sqrt(481)
sqrt(606)
sqrt(1024)
18^@
18^2
17^2
1 / 40
0.6/1
1/0.6
0.6 / 40
320 * 6.11
a = 1 * 10 ^ 20
a = 1 * 10 ^ 200
a = 1 * 10 ^ 2000
a
2 * 1012
2 ^ 1024
2 ^ 1023
2 ^ 1024
a = 2 ^ 1024
a = 2 ^ 1023
a = 2 ^ 1024
10 ^ 1
10 ^ 2
10 ^ 0
setwd("/Users/Qihong/Desktop/711PDP/net/")
