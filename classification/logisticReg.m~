%% Logistic regression classifier
% it TAKES a data set and a cross-validation block
% it RETURNS the cross-validated accuracy
function [accuracy, meanResponse, meanDeviation] = logisticReg(data, CVB, variance, spaBlur, showresults)
%% obtain the predictors and responses
X = data(: , 1 : (size(data,2) - 1));
y = data(:, size(data,2));

%% injecting normal noise (this might be important!)
X = X + normrnd(0,variance, size(X));
if spaBlur
    X = mean(X,2);
end

%% separate the training and testing sets
Xtest = X(CVB,:);
ytest = y(CVB,:);
Xtrain = X(~CVB,:);
ytrain = y(~CVB,:);

%% Compute Cost and Gradient using fminunc
%  Setup the data matrix appropriately, and add ones for the intercept term
[m, n] = size(Xtrain);
% Add the intercept term to the design matrix
Xtrain = [ones(m, 1) Xtrain];

% Initialize fitting parameters and options
initial_beta = zeros(n + 1, 1);
options = optimset('GradObj', 'on', 'MaxIter', 400);
%  Run fminunc to obtain the optimal beta
if showresults
    disp('Weights estimation in progress...')
end
beta = fminunc(@(t)(costFunction(t, Xtrain, ytrain)), initial_beta, options);

%% Predict and Accuracies
% compute the prediction
Xtest = [ones(size(Xtest,1), 1) Xtest]; % add the intercept term
rawPrediction = sigmoid(Xtest * beta);
predictedLabels  = rawPrediction >= 0.5;

% deviation is the L1 norm of the difference between prediction values and 
% truth, this is more sensitive than accuracy
deviation = sum(abs(rawPrediction - ytest));
accuracy = mean(double(predictedLabels == ytest)) * 100;

%% compute the mean 
meanResponse = sum(rawPrediction) / length(rawPrediction);
meanDeviation = deviation / length(rawPrediction);


%% show the results
if showresults
    % print the comparison between model response and the truth
    [rawPrediction , ytest]
    % Compute accuracy on our training set    
    fprintf('Cross-validated Accuracy: %.3f\n', accuracy);
    disp('Done!')
end

end



