#  compare categorical perception along spectral vs. temporal dimensions using second-order orthogonal polynomial
#  which terms show significant effects of dimension type? (model comparisons)
#  estimate parameter-specific p-values using normal approximation and Satterthwaite approximation (lmerTest): to what extent do model comparisons and the two parameter-specific approaches yield the same results?
#  plot observed and model fit data
#
# Exercise 3: analyze the combined effects of task difficulty and impairment (alcohol) on motor learning (MotorLearning)
#  plot the observed data
#  run a basic GCA with third-order orthogonal polynomials
#  re-code variables to get main effects instead of simple effects (i.e., set factor contrasts to "sum")
#  re-run GCA and compare results
##########################
summary(WordLearnEx)
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(fun.y=mean, geom="line")
library("minqa", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
t <- poly(1:10, 2)
t
plot(t)
t(1)
t[1]
t[1:end, 1]
t[1:, 1]
t[1:10, 1]
t <- poly(1:max(WordLearnEx$Block), 2)
t
#add it into the data frame
t
WordLearnEx[, paste("ot", 1:2, sep="")] <- t[WordLearnEx$Block, 1:2]
#re-check data
summary(WordLearnEx)
#orthogonal polynomial time
ggplot(WordLearnEx, aes(Block, ot1)) + stat_summary(fun.y=mean, geom="line")
last_plot() + stat_summary(aes(y=ot2), fun.y=mean, geom="line", color="red")
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
#parameter estimates
summary(m.2)
coefs <- data.frame(coef(summary(m.2)))
#parameter-specific p-values: use normal approximation
coefs$p <- 2*(1-pnorm(abs(coefs$t.value)))
coefs
#Alternative: use lmerTest to get Satterthwaite approximation
nal polynomial time
ggplot(WordLearnEx, aes(Block, ot1)) + stat_summary(fun.y=mean, geom="line")
last_plot() + stat_summary(aes(y=ot2), fun.y=mean, geom="line", color="red")
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(fun.y=mean, geom="line")
summary(WordLearnEx)
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(fun.y=mean, geom="line")
m.base <- lmer(Accuracy ~ (ot1+ot2) + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
m.0 <- lmer(Accuracy ~ (ot1+ot2) + TP + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
#add effect on slope
m.1 <- lmer(Accuracy ~ (ot1+ot2) + TP + TP:ot1 + (ot1 + ot2 | Subject), data=WordLearnEx, REML=F)
#add effect on quadratic
m.2 <- lmer(Accuracy ~ (ot1+ot2)*TP + (ot1+ot2 | Subject), data=WordLearnEx, REML=F)
#model comparisons
anova(m.base, m.0, m.1, m.2)
#plot model fit
ggplot(WordLearnEx, aes(Block, Accuracy, color=TP)) +
stat_summary(fun.data=mean_se, geom="pointrange") +
stat_summary(aes(y=fitted(m.2)), fun.y=mean, geom="line")
library(lmerTest)
install.packages('lmerTest')
library(lmerTest)
m.2t <- lmer(Accuracy ~ (ot1+ot2)*TP + (ot1+ot2 | Subject), data=WordLearnEx, REML=F)
summary(m.2t)
m.2t <- lmer(Accuracy ~ (ot1+ot2)*TP + (ot1+ot2 | Subject), data=WordLearnEx, REML=F)
summary(m.2t)
# Con: your model is now a lmerTest object rather than a lmerMod object and has somewhat different behavior
detach("package:lmerTest", unload=T)
source('~/Code/R/testing.R')
head(ChickWeight)
ggplot(TargetFix, aes(Time, meanFix, color=Condition)) +
stat_summary(fun.y=mean, geom="line") +
stat_summary(aes(fill=Condition), fun.data=mean_se, geom="ribbon", color=NA, alpha=0.3) +
theme_bw(base_size=10) + expand_limits(y=c(0,1)) +
labs(y="Fixation Proportion", x="Time since word onset (ms)")
ggplot(TargetFix, aes(Time, meanFix, color=Condition)) +
stat_summary(fun.y=mean, geom="line") +
stat_summary(aes(fill=Condition), fun.data=mean_se, geom="ribbon", color=NA, alpha=0.3) +
theme_bw(base_size=10) + expand_limits(y=c(0,1)) +
labs(y="Fixation Proportion", x="Time since word onset (ms)")
t <- poly(1:max(TargetFix$timeBin), 3)
#add it into data frame
TargetFix[,paste("ot", 1:3, sep="")] <- t[TargetFix$timeBin, 1:3]
#fit full model
m.full <- lmer(meanFix ~ (ot1+ot2+ot3)*Condition + #fixed effects
(ot1+ot2+ot3 | Subject) +
(ot1+ot2+ot3 | Subject:Condition), #random effects
data=TargetFix, REML=F)
summary(m.full)
install.packages('glmnet')
ver
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
}
celsius = function(212)
function(212)
celsius = function(fahrenheit) {
celsius = (fahrenheir - 32) * 5 / 9
}
stopifnot(isTRUE(all.equal(celsius(32), 0)))
celsius = function(fahrenheit) {
celsius = (fahrenheit - 32) * 5 / 9
}
stopifnot(isTRUE(all.equal(celsius(32), 0)))
stopifnot(isTRUE(all.equal(celsius(212), 100)))
celsius(32)
a = celsius(32)
a
a = celsius(212)
a
x = 1:10
x
x = (1:10, sep = 0.1)
x = seq(1,10,0.1)
x
y = 1 / (1 + exp(-x))
plot(x,y)
x = seq(-10, 10, 0.1)
plot(x,y)
x = seq(-10, 10, 0.1)
y = 1 / (1 + exp(-x))
plot(x,y)
plot(x,y,'l')
plot(x,y)
x = rnorm(100)
y = rnorm(100) + x
plot(x,y)
model = lm(y~x)
plot(model)
model
lm
a = 10
a.model = 10
a
a.mode
a.model
a
source('stats327_hw1.R')
675 * 5
675 + 1400 + 720
675 * 4
12 / 1.6
3600 + 40 * 60 + 35
10 / 0.71
2700 - 1885
x = rnorm(100)
y = rnorm(100) + x
plot(x,y)
lm(x,y)
lm(x~y)
m = lm(x~y)
abline(m)
m = lm(y ~ x)
abline(m)
plot(x,y)
abline(m, 'red')
abline(m, 'r')
x = seq(10,1)
x
x = seq(1,10,1)
x
x = seq(1,10,0.1)
x
x = seq(1,10,0.01)
y = 1 / (1 + e^(-x))
y = 1 / (1 + exp(1)^(-x))
exp(1)
plot(x,y)
x = seq(-10,10,0.01)
y = 1 / (1 + exp(1)^(-x))
plot(x,y)
? plt
? plot
plot(x,y,'l')
plot(x,y,'l')
1
1 + 1/3
1 - 1/3
1 - 1/3 + 1/5
1 - 1/3 - 1/5
(1 - 1/3 - 1/5)
4 * (1 - 1/3 - 1/5)
(1 - 1/3 - 1/5 + 1/7)
4 * (1 - 1/3 - 1/5 + 1/7)
4 * (1 - 1/3 - 1/5 + 1/7 - 1/9)
4 * (1 - 1/3 - 1/5 + 1/7 - 1/9 + 1/11)
1/ 7
f = rnorm(100)
f[3]
rep(2.3)
rep(2.3,3)
2.3 -> f[3]
f
seq(1,15,1)
5: 15
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = T)
sample(letters, 5 , replace = F)
sample(letters, 5 , replace = F)
sample(letters, 5 , replace = F)
sample(letters, 5 , replace = F)
sample(1:16, 5 , replace = T)
sample(1:16, 5 , replace = T)
c(5, letters)
c
c
c(5, letters)
c(5, letters)
c(5, letters)
c(5, letters)
c(5, letters)
sample(LETTERS, 5, replace = F)
sample(LETTERS, 5, replace = F)
sample(LETTERS, 5, replace = F)
a <- sample(letters[1:4], 100, replace=T)
a
a <- sample(letters[1:4], 100, replace=F)
a <- sample(letters[1:4], 100, replace=F)
a <- sample(letters[1:4], 100, replace=T)
table(a)
pie(table(a))
pie(a)
plot(a)
plot(table(a))
a =  14549535
a
a / 19
a / 18
a / 20
b = a
b = 692835
a
b
a / b
a
installed.packages
installed.packages()
0.0625 * 2
a = 0.0625 * 2
a * 3
1/50
1/50 * 0.06
1/50 * 0.06 * 100
install.packages(c("caTools", "class", "cluster", "codetools", "formatR", "Formula", "gplots", "Hmisc", "KernSmooth", "labeling", "lme4", "lmerTest", "markdown", "MASS", "mgcv", "mime", "multcomp", "mvtnorm", "pbkrtest", "Rcpp", "RcppEigen", "sandwich"))
#'
#' Can we assure ourselves that all the same principles apply?
#+ echo=FALSE
library(lme4)
library(dplyr)
library(AICcmodavg)
update
update()''
update()
VERSION
version
version
version
#+ echo=FALSE
library(lme4)
library(dplyr)
library(AICcmodavg)
library(ggplot2)
inta
install.packages('Matrix')
install.packages("Matrix")
#+ echo=FALSE
library(lme4)
library(dplyr)
library(AICcmodavg)
library(ggplot2)
installed.packages
installed.packages()
#+ echo=FALSE
library(lme4)
library(lme4)
library(dplyr)
library(dplyr)
library(AICcmodavg)
library(ggplot2)
#' ## Same data
df <- sleepstudy
df$Subject <- paste0("S", df$Subject) ## don't accidentally treat this as a num
raw_data <- ggplot(df, aes(x = Days, y = Reaction)) +
facet_wrap("Subject", ncol = 3) +
geom_point()
raw_data
#' ## By subject plots
#' or: "Look what ggplot can do!"
gg_magic <- raw_data +
geom_smooth(method = "lm", formula = y ~ x)
gg_magic
#' Look at the effect for each subject
40
60 - 7
53 / 3
53 / 3 * 100
53 / 3 * 5
14 * 0.71
ls
ls()
rm()
rm(ls())
? rm()
rm (list = ls())
ls
? sep()
x = 1 : 10
x
x = 1:0.1:10
x
x = 1:10, sep = 0.1
x = (1:10, sep = 0.1)
x = 1:10:0.1
x
? plot
require(stats)
plot(cars)
lines(lowess(cars))
? seq
x = seq(1,10)
x
x = seq(1,10,1)
x
x = seq(1,10,0.1)
x
x = seq(1,10,0.01)
y = 1 / (1 + (exp(-x)))
plot (x,y)
x = seq(-10,10,0.01)
y = 1 / (1 + (exp(-x)))
plot (x,y)
plot.net()
plot.new()
? plot
plot (x,y, "p")
plot.new()
plot (x,y, "l")
x = 0.45
y = 1 / (1 + (exp(-x)))
y
y + 0.08
x = y + 0.08
y = 1 / (1 + (exp(-x)))
y
x = 0.28
y = 1 / (1 + (exp(-x)))
y
x = 0.28
y = 1 / (1 + (exp(-x)))
y
bias = y
bias
x = -0.28
y = 1 / (1 + (exp(-x)))
h1 = y
x = -0.4
y = 1 / (1 + (exp(-x)))
h2 = y
h1
h1 + h2 + bias
x = (h1 + h2 + bias)
y = 1 / (1 + (exp(-x)))
y
sqrt(25)
sqrt(155)
sqrt(225)
sqrt(481)
sqrt(606)
sqrt(1024)
18^@
18^2
17^2
1 / 40
0.6/1
1/0.6
0.6 / 40
320 * 6.11
a = 1 * 10 ^ 20
a = 1 * 10 ^ 200
a = 1 * 10 ^ 2000
a
2 * 1012
2 ^ 1024
2 ^ 1023
2 ^ 1024
a = 2 ^ 1024
a = 2 ^ 1023
a = 2 ^ 1024
setwd("~/Desktop/711PDP/net/simple")
rm(list = ls())
# load the data
temp = read.table("hiddenOutFinal.txt")
# convert the hidden activation to a matrix
hiddenData = as.matrix(temp[,4:length(temp)])
row.names(hiddenData) = temp[,2]
# split the hidden activation for visual input and verbal input
verbalHidden = hiddenData[1:16,]
visualHidden = hiddenData[17:32,]
# verbal animals bar plots
plot.new()
par(mfrow = c(2,4))
for (i in 1:8) {
barplot(hiddenData[i,], beside = T)
title(row.names(hiddenData)[i])
}
# verbal
artifacts bar plots
plot.new()
par(mfrow = c(2,4))
for (i in 1:8) {
barplot(hiddenData[i + 8,], beside = T)
title(row.names(hiddenData)[i + 8])
}
# you can also plot visual bar plots
# ...
# Analysis for full data (verbal + visual)
# compute the dissimilarity structure
plot.new()
par(mfrow = c(1,1))
distanceMatrix = (as.matrix(dist(hiddenData)))
image(distanceMatrix[16:1,], zlim = c(0,2), col = heat.colors(10, 1), yaxt = "n", xaxt = "n")
# mtext(row.names(hiddenData))
lowerTriangularIndices = lower.tri(distanceMatrix)
range (distanceMatrix[lowerTriangularIndices])
image(distanceMatrix[16:1,], zlim = c(0,3.5), col = heat.colors(10, 1), yaxt = "n", xaxt = "n")
# hclust
plot.new()
par(mfrow = c(1,1))
plot(hclust(dist(hiddenData)))
# 2D MDS
hiddenMDS = cmdscale(distanceMatrix)
plot(hiddenMDS)
text(hiddenMDS,labels = row.names(hiddenData))
## analysis for verbal data only
# compute the distance matrix
distanceMatrix = (as.matrix(dist(verbalHidden)))
# heat plot
plot.new()
par(mfrow = c(1,1))
image(distanceMatrix[16:1,], zlim = c(0,2), col = heat.colors(10, 1), yaxt = "n", xaxt = "n")
lowerTriangularIndices = lower.tri(distanceMatrix)
range (distanceMatrix[lowerTriangularIndices])
image(distanceMatrix[16:1,], zlim = c(0,3.5), col = heat.colors(10, 1), yaxt = "n", xaxt = "n")
# hclust
plot.new()
par(mfrow = c(1,1))
plot(hclust(dist(verbalHidden)))
# 2D MDS
hiddenMDS = cmdscale(distanceMatrix)
setwd("~/Desktop/711PDP/net/sim1_simple")
rm(list = ls())
# load the data
temp = read.table("hiddenOutFinal.txt")
# convert the hidden activation to a matrix
hiddenData = as.matrix(temp[,4:length(temp)])
row.names(hiddenData) = temp[,2]
# split the hidden activation for visual input and verbal input
verbalHidden = hiddenData[1:16,]
visualHidden = hiddenData[17:32,]
# verbal animals bar plots
plot.new()
par(mfrow = c(2,4))
for (i in 1:8) {
barplot(hiddenData[i,], beside = T)
title(row.names(hiddenData)[i])
}
plot.new()
par(mfrow = c(1,1))
distanceMatrix = (as.matrix(dist(hiddenData)))
image(distanceMatrix[16:1,], zlim = c(0,2), col = heat.colors(10, 1), yaxt = "n", xaxt = "n")
# mtext(row.names(hiddenData))
lowerTriangularIndices = lower.tri(distanceMatrix)
range (distanceMatrix[lowerTriangularIndices])
image(distanceMatrix[16:1,], zlim = c(0,3.5), col = heat.colors(10, 1), yaxt = "n", xaxt = "n")
# hclust
plot.new()
par(mfrow = c(1,1))
plot(hclust(dist(hiddenData)))
hiddenMDS = cmdscale(distanceMatrix)
plot(hiddenMDS)
text(hiddenMDS,labels = row.names(hiddenData))
## analysis for verbal data only
distanceMatrix = (as.matrix(dist(verbalHidden)))
# heat plot
plot.new()
par(mfrow = c(1,1))
image(distanceMatrix[16:1,], zlim = c(0,2), col = heat.colors(10, 1), yaxt = "n", xaxt = "n")
lowerTriangularIndices = lower.tri(distanceMatrix)
range (distanceMatrix[lowerTriangularIndices])
image(distanceMatrix[16:1,], zlim = c(0,3.5), col = heat.colors(10, 1), yaxt = "n", xaxt = "n")
# hclust
plot.new()
par(mfrow = c(1,1))
plot(hclust(dist(verbalHidden)))
